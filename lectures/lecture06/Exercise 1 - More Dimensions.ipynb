{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1\n",
    "====\n",
    "\n",
    "In this exercise you should extend the `fit_np` implementation of the perceptron to allow an arbitrary number of features, not just two. We provide a data set generator `make_separable_data`, which takes a dimensionality parameter. By default the parameter is set to 5. \n",
    "\n",
    "Note that as you move beyond two (and three) dimensions, you won't be able to plot the dataset and the decision boundary anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def make_separable_data(num, dimensions=5, epsilon=0.001):\n",
    "    '''\n",
    "    generates a linearly separable data set\n",
    "    '''\n",
    "    # fill the first column (the labels) randomly with -1s and 1s\n",
    "    labels = 2 * np.random.randint(0, 2, num) - 1\n",
    "    # pick x1 at random\n",
    "    x1 = np.random.random(num)\n",
    "    # base x2 off of x1, add random noise and epsilon, and move up or down\n",
    "    x2 = x1 + ((epsilon + np.random.random(num)) * labels)\n",
    "\n",
    "    X = np.vstack((x1,x2)).T\n",
    "\n",
    "    y = np.random.rand(2, dimensions)\n",
    "    X2 = np.dot(X,y)\n",
    "    \n",
    "    return pd.DataFrame(X2, columns=['x%s' % str(i+1) for i in range(dimensions)]), labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `fit_np` function in the Perceptron notebook below. Then modify the implementation to accept a dataset of arbitrary dimensionality. \n",
    "\n",
    "Note that you can get a `numpy` array from a `DataFrame` in two ways\n",
    "\n",
    "* Call `np.array()` with the `DataFrame` as an argument\n",
    "* User the `.values()` function on the `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your changes here\n",
    "\n",
    "def fit_np(train_data, train_labels, num_epochs=20):\n",
    "    '''\n",
    "    fit the model to a data set\n",
    "    '''\n",
    "    N = train_labels.shape[0]\n",
    "    \n",
    "    # set weights and bias\n",
    "    weights = np.random.rand(3) * 0.01 - 0.05\n",
    "    \n",
    "    learning_rate = 0.5\n",
    "    \n",
    "    # add bias term\n",
    "    train_data['bias'] = 1\n",
    "\n",
    "    # iterate for X epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # go over each instance\n",
    "        for i in range(N):\n",
    "            instance = np.array(train_data.ix[i])\n",
    "            label = train_labels[i]\n",
    "        \n",
    "            # compute activation\n",
    "            activation = np.dot(instance, weights)\n",
    "    \n",
    "            # check whether we have to update\n",
    "            if np.sign(activation) != label:\n",
    "                weights += learning_rate * instance * label\n",
    "                plot_decision_boundary(weights[0], weights[1], weights[2])\n",
    "                \n",
    "        print(\"iteration\", epoch+1)\n",
    "        \n",
    "    print()\n",
    "    return weights\n",
    "        \n",
    "\n",
    "def predict_np(test_data, test_labels, weights):\n",
    "    '''\n",
    "    compute predicted labels for a data set goven a model\n",
    "    '''    \n",
    "    N = test_labels.shape[0]\n",
    "\n",
    "    # add bias term\n",
    "    test_data['bias'] = 1\n",
    "\n",
    "    predictions = np.sign(np.dot(np.array(test_data), weights))\n",
    "    correct_predictions = (predictions == test_labels)\n",
    "\n",
    "    return predictions, correct_predictions.sum() / float(N)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, labels = make_separable_data(1000, epsilon=0.001)\n",
    "train_data = data[:800]\n",
    "train_labels = labels[:800]\n",
    "test_data = data[800:]\n",
    "test_labels = labels[800:]\n",
    "\n",
    "weights = fit_np(train_data, train_labels) \n",
    "train_predictions, train_accuracy = predict_np(train_data, train_labels, weights)\n",
    "print(\"Train\", train_accuracy)\n",
    "\n",
    "test_predictions, test_accuracy = predict_np(test_data, test_labels, weights)\n",
    "print(\"Test\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
